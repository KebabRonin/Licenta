{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep GPs and DSPPs w/ Multiple Outputs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this example, we will demonstrate how to construct deep GPs that can model vector-valued functions (e.g. multitask/multi-output GPs).\n",
    "\n",
    "This tutorial can also be used to construct multitask [deep sigma point processes](./Deep_Sigma_Point_Processes.ipynb) by replacing `DeepGPLayer`/`DeepGP`/`DeepApproximateMLL` with `DSPPLayer`/`DSPP`/`DeepPredictiveLogLikelihood`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import math\n",
    "import gpytorch\n",
    "from torch.nn import Linear\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution, \\\n",
    "    LMCVariationalStrategy\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.mlls import DeepApproximateMLL, VariationalELBO\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training data\n",
    "\n",
    "In the next cell, we set up the training data for this example. We'll be using 100 regularly spaced points on [0,1] which we evaluate the function on and add Gaussian noise to get the training labels.\n",
    "\n",
    "We'll have four functions - all of which are some sort of sinusoid. Our `train_targets` will actually have two dimensions: with the second dimension corresponding to the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_utils\n",
    "import torch.utils.data as tdata\n",
    "dset = my_utils.CustomSQLDataset(norm_method='none')\n",
    "splits = my_utils.get_splits()\n",
    "# print(splits)\n",
    "trs = tdata.random_split(dset, splits, generator=torch.Generator().manual_seed(50))\n",
    "sqldloader = tdata.DataLoader(trs[0], num_workers=0, batch_sampler=tdata.BatchSampler(tdata.RandomSampler(trs[0]), batch_size=100, drop_last=False), collate_fn=my_utils.identity)\n",
    "train_x, train_y = next(iter(sqldloader))\n",
    "train_x, train_y = train_x.to('cuda'), train_y.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a multitask deep GP\n",
    "\n",
    "The layers of a multitask deep GP will look identical to the layers of a [single-output deep GP](./Deep_Gaussian_Processes.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a simple standard layer\n",
    "\n",
    "class DGPHiddenLayer(DeepGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=128, linear_mean=True):\n",
    "        inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "        batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy, input_dims, output_dims)\n",
    "        self.mean_module = ConstantMean() if linear_mean else LinearMean(input_dims)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            MaternKernel(nu=2.5, batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape, ard_num_dims=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main body of the deep GP will look very similar to the single-output deep GP, with a few changes.\n",
    "\n",
    "**Most importantly** - the last layer will have `output_dims=num_tasks`, rather than `output_dims=None`. As a result, the output of the model will be a `MultitaskMultivariateNormal` rather than a standard `MultivariateNormal` distribution.\n",
    "\n",
    "There are two other small changes, which are noted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = 368 #train_y.size(1)\n",
    "num_hidden_dgp_dims = 3\n",
    "\n",
    "\n",
    "class MultitaskDeepGP(DeepGP):\n",
    "    def __init__(self, train_x_shape):\n",
    "        hidden_layer = DGPHiddenLayer(\n",
    "            input_dims=train_x_shape[-1],\n",
    "            output_dims=num_hidden_dgp_dims,\n",
    "            linear_mean=True\n",
    "        )\n",
    "        last_layer = DGPHiddenLayer(\n",
    "            input_dims=hidden_layer.output_dims,\n",
    "            output_dims=num_tasks,\n",
    "            linear_mean=False\n",
    "        )\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "        # We're going to use a ultitask likelihood instead of the standard GaussianLikelihood\n",
    "        self.likelihood = MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_rep1 = self.hidden_layer(inputs)\n",
    "        output = self.last_layer(hidden_rep1)\n",
    "        return output\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # The output of the model is a multitask MVN, where both the data points\n",
    "            # and the tasks are jointly distributed\n",
    "            # To compute the marginal predictive NLL of each data point,\n",
    "            # we will call `to_data_independent_dist`,\n",
    "            # which removes the data cross-covariance terms from the distribution.\n",
    "            preds = model.likelihood(model(test_x)).to_data_independent_dist()\n",
    "\n",
    "        return preds.mean.mean(0), preds.variance.mean(0)\n",
    "\n",
    "\n",
    "model = MultitaskDeepGP(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and making predictions\n",
    "\n",
    "This code should look similar to the DGP training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [03:19<00:00,  9.99s/it, loss=2.29e+3]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = DeepApproximateMLL(VariationalELBO(model.likelihood, model, num_data=train_y.size(0)))\n",
    "\n",
    "num_epochs = 1 if smoke_test else 20\n",
    "epochs_iter = tqdm.trange(num_epochs, desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    epochs_iter.set_postfix(loss=loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_x, train_y, optimizer, mll, loss, epochs_iter\n",
    "# del test_x, test_y\n",
    "# del model\n",
    "# Make predictions\n",
    "# model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"dgp_model.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqldloader = tdata.DataLoader(trs[-1], num_workers=0, batch_sampler=tdata.BatchSampler(tdata.RandomSampler(trs[-1]), batch_size=2, drop_last=False), collate_fn=my_utils.identity)\n",
    "test_x, test_y = next(iter(sqldloader))\n",
    "# print(test_x.device)\n",
    "# test_x, test_y = test_x.to('cuda'), test_y.to('cuda')\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(model, open(\"dgp_model.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultitaskDeepGP' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m k \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# import gc\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KebabWarrior\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultitaskDeepGP' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "# torch.cuda.empty_cache()\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float16)\n",
    "# Make predictions\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "torch.cuda.empty_cache()\n",
    "# test_x.to('cuda')\n",
    "with torch.no_grad():#, gpytorch.settings.fast_pred_var():\n",
    "    mean, var = model.predict(test_x)\n",
    "    print(mean)\n",
    "    # lower = mean - 2 * var.sqrt()\n",
    "    # upper = mean + 2 * var.sqrt()\n",
    "\n",
    "# # Plot results\n",
    "# fig, axs = plt.subplots(1, num_tasks, figsize=(4 * num_tasks, 3))\n",
    "# for task, ax in enumerate(axs):\n",
    "#     ax.plot(train_x.squeeze(-1).detach().numpy(), train_y[:, task].detach().numpy(), 'k*')\n",
    "#     ax.plot(test_x.squeeze(-1).numpy(), mean[:, task].numpy(), 'b')\n",
    "#     ax.fill_between(test_x.squeeze(-1).numpy(), lower[:, task].numpy(), upper[:, task].numpy(), alpha=0.5)\n",
    "#     ax.set_ylim([-3, 3])\n",
    "#     ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "#     ax.set_title(f'Task {task + 1}')\n",
    "# fig.tight_layout()\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
